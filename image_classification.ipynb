{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Load data using Python's `open` function to read `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# open csv file\n",
    "with open('ds_challenge_train.csv', 'r') as f:\n",
    "    data = f.readlines()\n",
    "# remove last character `\\n`\n",
    "data = np.array([x[:-1].split(',') for x in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# split each rows into label and features\n",
    "X = data[:,1:]\n",
    "y = data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Convert `X` into an image by reshaping data into square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 20000 samples, with image size of 28x28 pixels\n"
    }
   ],
   "source": [
    "N, M = X.shape\n",
    "# N is number of samples, M is image size\n",
    "# since the image is a square, we can obtain the width, height of the image by taking square root if M\n",
    "size = int(M**0.5)\n",
    "print('There are {} samples, with image size of {}x{} pixels'.format(N, size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X = data[:,1:].reshape(N, size, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "However, data type of `X` is still `str`, we need to convert it into `int` data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a16b65c9c85a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# convert X into `int` type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '-'"
     ]
    }
   ],
   "source": [
    "# convert X into `int` type\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Notice that this blocks create an error as there is a missing value interpreted as `-`. In this case, we will replace this value with `0` which denotes black pixel. In other word, we can see missing value as a black pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X = np.where(X == '-', '0', X) # replace '-' with '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4b051728c346>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# convert to int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "X = X.astype(int) # convert to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Still, there is a missing values which is an empty string `''`. Again, we will fill it with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X = np.where(X == '', '0', X) # replace '-' with '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X = X.astype(int) # convert to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y = y.astype(int) # convert label to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, the code looks fine! It is time to normalize the data by dividing image by 255. We divide by 255 to ensure that the image input values range between 0 to 1 as the pixels range are between 0 to 255 (8-bit resolution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X = X/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, let's inspect the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"246.958125pt\" version=\"1.1\" viewBox=\"0 0 231.84 246.958125\" width=\"231.84pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 246.958125 \r\nL 231.84 246.958125 \r\nL 231.84 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#pef55ef41eb)\">\r\n    <image height=\"218\" id=\"imaged747b613aa\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-21.758125\"/>\r\n   </g>\r\n   <g id=\"text_1\">\r\n    <!-- Label: 6 -->\r\n    <defs>\r\n     <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n     <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n     <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n     <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n     <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n     <path d=\"M 11.71875 12.40625 \r\nL 22.015625 12.40625 \r\nL 22.015625 0 \r\nL 11.71875 0 \r\nz\r\nM 11.71875 51.703125 \r\nL 22.015625 51.703125 \r\nL 22.015625 39.3125 \r\nL 11.71875 39.3125 \r\nz\r\n\" id=\"DejaVuSans-58\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n     <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n    </defs>\r\n    <g transform=\"translate(91.986563 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"55.712891\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"116.992188\" xlink:href=\"#DejaVuSans-98\"/>\r\n     <use x=\"180.46875\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"241.992188\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"269.775391\" xlink:href=\"#DejaVuSans-58\"/>\r\n     <use x=\"303.466797\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"335.253906\" xlink:href=\"#DejaVuSans-54\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pef55ef41eb\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"7.2\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "idx=0\n",
    "plt.imshow(X[idx], cmap='gray')\n",
    "plt.title('Label: {}'.format(y[idx]))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Since we will use CNN in our model (see `Modeling` section later), the input shape should be 3 dimensional. Therefore, we will increase the dimension using `np.expand_dims`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# convert shape from (20000, 28, 28) to (20000, 28, 28, 1)\n",
    "X = np.expand_dims(X, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We do a classic train/test split using 70% training and 30% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split training set using `train_test_split`\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To make a prediction model, we use a simple Convolutional Neural Networks (CNN) with 3 layers following by Fully conected layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model\n",
    "### Convolutional Neural Networks (CNN)\n",
    "CNN is a type of neural network which use the concept of convolution to find matched filter. The CNN learns the **filters** which will be used to convolved with the input signal (which is an input image in this case) and learn the best filters that will find a pattern. This type of neural network typically used together with **Max Pooling Layer**\n",
    "\n",
    "### Max Pooling Layer\n",
    "This layers allow the feature map (output from CNN) to be reduced in size. Which makes the later CNN layer have a larger receptive field with less computational cost.\n",
    "\n",
    "### Fully Connected (FC)\n",
    "Fully connected layer is comparable to Multilayer Perceptron which mimics the use of human brain. It connected with a mesh of connection between input and output nodes where the model learns to find a proper weight connection between input and output nodes.\n",
    "\n",
    "## Optimization\n",
    "Both CNN and FC learn it weights (and filters) using a gradient-based optimization method which is backpropagation algorithm. We can choose which optimizer to use from tensorflow's library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Architecture\n",
    "We use 3 layers of convolution and max pool layers before flatten the feature maps to be a vector. After that, the vectorized feature map is passed to a fully connected layer with 128 units before passing to a classification layer (softmax layer) which emits output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\byth-\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 28, 28, 8)         80        \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 14, 14, 8)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 14, 14, 16)        1168      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 7, 7, 16)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 3, 3, 32)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 288)               0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               36992     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 44,170\nTrainable params: 44,170\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential([\n",
    "    Conv2D(filters=8, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Flatten(), # convert feature maps into a vector\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(1e-4), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/20\n14000/14000 [==============================] - 5s 335us/sample - loss: 2.0918 - acc: 0.4416\nEpoch 2/20\n14000/14000 [==============================] - 4s 301us/sample - loss: 0.8725 - acc: 0.7978\nEpoch 3/20\n14000/14000 [==============================] - 4s 294us/sample - loss: 0.4434 - acc: 0.8780\nEpoch 4/20\n14000/14000 [==============================] - 4s 295us/sample - loss: 0.3259 - acc: 0.9090\nEpoch 5/20\n14000/14000 [==============================] - 4s 318us/sample - loss: 0.2655 - acc: 0.9241\nEpoch 6/20\n14000/14000 [==============================] - 4s 319us/sample - loss: 0.2268 - acc: 0.9354\nEpoch 7/20\n14000/14000 [==============================] - 4s 302us/sample - loss: 0.2010 - acc: 0.9412\nEpoch 8/20\n14000/14000 [==============================] - 4s 312us/sample - loss: 0.1828 - acc: 0.9466\nEpoch 9/20\n14000/14000 [==============================] - 4s 321us/sample - loss: 0.1685 - acc: 0.9491\nEpoch 10/20\n14000/14000 [==============================] - 4s 298us/sample - loss: 0.1552 - acc: 0.9546\nEpoch 11/20\n14000/14000 [==============================] - 4s 311us/sample - loss: 0.1449 - acc: 0.9579\nEpoch 12/20\n14000/14000 [==============================] - 4s 305us/sample - loss: 0.1365 - acc: 0.9589\nEpoch 13/20\n14000/14000 [==============================] - 4s 306us/sample - loss: 0.1302 - acc: 0.9621\nEpoch 14/20\n14000/14000 [==============================] - 4s 305us/sample - loss: 0.1231 - acc: 0.9626\nEpoch 15/20\n14000/14000 [==============================] - 4s 301us/sample - loss: 0.1174 - acc: 0.9646\nEpoch 16/20\n14000/14000 [==============================] - 4s 317us/sample - loss: 0.1115 - acc: 0.9666\nEpoch 17/20\n14000/14000 [==============================] - 4s 264us/sample - loss: 0.1074 - acc: 0.9674\nEpoch 18/20\n14000/14000 [==============================] - 4s 310us/sample - loss: 0.1030 - acc: 0.9685\nEpoch 19/20\n14000/14000 [==============================] - 6s 444us/sample - loss: 0.1002 - acc: 0.9695\nEpoch 20/20\n14000/14000 [==============================] - 6s 396us/sample - loss: 0.0952 - acc: 0.9716\n"
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1) # get prediction\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model Accuracy: 96.25%\n"
    }
   ],
   "source": [
    "print('Model Accuracy: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "According to the result, 96.25% seems to be an acceptable accuracy. However, there is still a lot of room to be improved for this model using the following techniques:\n",
    "- Data Augmentation\n",
    "- Batch Normalization\n",
    "- Dropout\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}